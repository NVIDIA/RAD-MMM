{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5c0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to RAD-MMM inference tutorial\n",
    "\n",
    "## imports\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append('/akshit/scratch/RAD-MMM/vocoders')\n",
    "sys.path.append('/akshit/scratch/RAD-MMM')\n",
    "from pytorch_lightning.cli import LightningCLI\n",
    "from tts_lightning_modules import TTSModel\n",
    "from data_modules import BaseAudioDataModule\n",
    "from jsonargparse import lazy_instance\n",
    "from decoders import RADMMMFlow\n",
    "from loss import RADTTSLoss\n",
    "import inspect\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from training_callbacks import LogDecoderSamplesCallback, \\\n",
    "    LogAttributeSamplesCallback\n",
    "from utils import get_class_args\n",
    "from tts_text_processing.text_processing import TextProcessing\n",
    "from common import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2738468b-2171-4137-9c28-4702b5db9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tts_main import RADTTSLightningCLI,lcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05e7b4-1561-4cf0-9953-50ea676ab33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6c72a3-2e28-47c6-bb9d-4e0b53845456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "attribute_model_path = \"/akshit/scratch/generator_ckpt/radmmm_public/attribute_model.ckpt\"\n",
    "decoder_model_path = \"/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\"\n",
    "gen_config_path = \"/akshit/scratch/generator_ckpt/radmmm_public/config_interactive.yaml\"\n",
    "voc_model_path = \"/akshit/scratch/generator_ckpt/hfg_public/g_00072000\"\n",
    "voc_config_path = \"/akshit/scratch/generator_ckpt/hfg_public/config_16khz.json\"\n",
    "phonemizer_cfg='{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5356651-8a00-4440-acb9-5257428620fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f90560-816d-4c65-b59a-915b28fc1a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import mock\n",
    "\n",
    "# with mock.patch(\"sys.argv\", [\"predict\", \"--config=\"+gen_config_path,\\\n",
    "#                              # \"--ckpt_path=\"+attribute_model_path,\\\n",
    "#                              \"--model.predict_mode=tts\", \\\n",
    "#                              \"--data.inference_transcript=model_inputs/resynthesis_prompts.json\", \\\n",
    "#                              \"--model.prediction_output_dir=/akshit/scratch/RAD-MMM/tutorials/out1\", \\\n",
    "#                              \"--trainer.devices=1\", \"--data.batch_size=1\", \\\n",
    "#                              \"--model.vocoder_checkpoint_path=\"+voc_model_path, \\\n",
    "#                              \"--model.vocoder_config_path=\"+voc_config_path, \\\n",
    "#                              \"--data.phonemizer_cfg=\"+'{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}', \\\n",
    "#                              \"--model.encoders_path=\"+decoder_model_path, \\\n",
    "#                              \"--model.decoder_path=\"+decoder_model_path, \\\n",
    "#                              \"--model.output_directory=/akshit/scratch/RAD-MMM/tutorials/run1\"]):\n",
    "#     cli = RADTTSLightningCLI(TTSModel, BaseAudioDataModule, save_config_kwargs={\"overwrite\": True}, run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bf7184-cc86-4fa1-9d6f-7306ccc624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results in issue - SIGSEV\n",
    "\n",
    "# cli.trainer.predict(model=cli.model,datamodule=cli.datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50cd070-2311-42d8-82ed-1352e3c96188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, state_dict: Mapping[str, Any], strict: bool = True)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempting to load the model\n",
    "import pytorch_lightning\n",
    "import inspect\n",
    "import torch\n",
    "from decoders import RADMMMFlow\n",
    "# inspect.signature(pytorch_lightning.cli.instantiate_module)\n",
    "inspect.signature(torch.nn.Module.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edddca9-4d40-456e-9a80-cc438859255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/akshit/scratch/RAD-MMM/tutorials/run1/lightning_logs/version_12/hparams.yaml\", \"r\") as f:\n",
    "    hparams = yaml.safe_load(f)\n",
    "\n",
    "with open(gen_config_path, \"r\") as f:\n",
    "    gen_config = yaml.safe_load(f)\n",
    "# hparams['decoder']['init_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b86dbb4-be11-412b-979b-ad13ec9e52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mauvilsa/lightning/blob/c71406f390b89be7bbe3415e57dd785095675779/src/lightning/pytorch/cli.py\n",
    "def instantiate_class(init):\n",
    "    \"\"\"Instantiates a class with the given args and init.\n",
    "\n",
    "    Args:\n",
    "        args: Positional arguments required for instantiation.\n",
    "        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\n",
    "\n",
    "    Returns:\n",
    "        The instantiated class object.\n",
    "    \"\"\"\n",
    "    kwargs = init.get(\"init_args\", {})\n",
    "    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\n",
    "    module = __import__(class_module, fromlist=[class_name])\n",
    "    args_class = getattr(module, class_name)\n",
    "    return args_class(**kwargs)\n",
    "    \n",
    "# https://lightning.ai/forums/t/best-way-to-use-load-from-checkpoint-when-model-contains-other-models/2094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15c16de-ca51-423f-a718-0a17ea93884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/akshit/scratch/RAD-MMM/common.py:557: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)\n",
      "  W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:1728: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "LU, pivots = torch.lu(A, compute_pivots)\n",
      "should be replaced with\n",
      "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "and\n",
      "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "should be replaced with\n",
      "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1991.)\n",
      "  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_loss\n",
      "text_encoder\n",
      "Applying spectral norm to text encoder LSTM\n",
      "f0_predictor\n",
      "f0_predictor_loss\n",
      "energy_predictor\n",
      "energy_predictor_loss\n",
      "voiced_predictor\n",
      "voiced_predictor_loss\n",
      "duration_predictor\n",
      "duration_predictor_loss\n",
      "speaker_embed_regularization_loss\n",
      "speaker_accent_cross_regularization_loss\n"
     ]
    }
   ],
   "source": [
    "ttsmodel_kwargs={}\n",
    "for k,v in hparams.items():\n",
    "    if type(v) == dict and 'class_path' in v:\n",
    "        print(k)\n",
    "        ttsmodel_kwargs[k] = instantiate_class(v)\n",
    "    elif k != \"_instantiator\":\n",
    "        ttsmodel_kwargs[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537eefd4-b1a5-42e5-b406-24360e90896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder': RADMMMFlow(\n",
       "   (length_regulator): LengthRegulator()\n",
       "   (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
       "   (flows): ModuleList(\n",
       "     (0): FlowStep(\n",
       "       (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2-3): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (4-5): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (6-7): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
       " ),\n",
       " 'decoder_loss': RADMMMLoss(\n",
       "   (attn_loss): AttentionLoss(\n",
       "     (attn_ctc_loss): AttentionCTCLoss(\n",
       "       (log_softmax): LogSoftmax(dim=3)\n",
       "       (CTCLoss): CTCLoss()\n",
       "     )\n",
       "     (attn_bin_loss): AttentionBinarizationLoss()\n",
       "   )\n",
       " ),\n",
       " 'text_encoder': Encoder(\n",
       "   (convolutions): ModuleList(\n",
       "     (0-2): 3 x Sequential(\n",
       "       (0): ConvNorm(\n",
       "         (conv): PartialConv1d(520, 520, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1): InstanceNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "     )\n",
       "   )\n",
       "   (lstm): LSTM(520, 260, batch_first=True, bidirectional=True)\n",
       " ),\n",
       " 'f0_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'f0_predictor_loss': AttributeRegressionLoss(),\n",
       " 'energy_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'energy_predictor_loss': AttributeRegressionLoss(),\n",
       " 'voiced_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'voiced_predictor_loss': AttributeRegressionLoss(),\n",
       " 'duration_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'duration_predictor_loss': AttributeRegressionLoss(),\n",
       " 'speaker_embed_regularization_loss': VarianceCovarianceEmbeddingRegLoss(),\n",
       " 'speaker_accent_cross_regularization_loss': AttributeMinCrossCovarianceRegLoss(),\n",
       " 'optim_algo': 'RAdam',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 1e-06,\n",
       " 'sigma': 1.0,\n",
       " 'iters_per_checkpoint': 3000,\n",
       " 'unfreeze_modules': 'all',\n",
       " 'binarization_start_iter': 0,\n",
       " 'output_directory': '/akshit/scratch/RAD-MMM/tutorials/run1',\n",
       " 'log_decoder_samples': True,\n",
       " 'scale_mel': True,\n",
       " 'vocoder_config_path': '/akshit/scratch/generator_ckpt/hfg_public/config_16khz.json',\n",
       " 'vocoder_checkpoint_path': '/akshit/scratch/generator_ckpt/hfg_public/g_00072000',\n",
       " 'p_estimate_ambiguous_phonemes': 0.0,\n",
       " 'phoneme_estimation_start_iter': 1000,\n",
       " 'decoder_path': '/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt',\n",
       " 'encoders_path': '/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt',\n",
       " 'f0_loss_voiced_only': True,\n",
       " 'n_speakers': 7,\n",
       " 'n_speaker_dim': 16,\n",
       " 'use_accent': True,\n",
       " 'n_accents': 7,\n",
       " 'n_accent_dim': 8,\n",
       " 'n_text_dim': 512,\n",
       " 'n_text_tokens': 426,\n",
       " 'lstm_norm_fn': 'spectral',\n",
       " 'n_mel_channels': 80,\n",
       " 'use_syncbnorm': False,\n",
       " 'prediction_output_dir': '/akshit/scratch/RAD-MMM/tutorials/out1',\n",
       " 'predict_mode': 'tts',\n",
       " 'use_accent_emb_for_encoder': True,\n",
       " 'use_accent_emb_for_decoder': False,\n",
       " 'use_accent_emb_for_alignment': False,\n",
       " 'use_speaker_emb_for_alignment': True,\n",
       " 'n_augmentations': 2,\n",
       " 'sampling_rate': 16000,\n",
       " 'symbol_set': 'radmmm_phonemizer_marker_segregated',\n",
       " 'cleaner_names': ['radtts_cleaners'],\n",
       " 'heteronyms_path': 'tts_text_processing/heteronyms',\n",
       " 'phoneme_dict_path': 'tts_text_processing/cmudict-0.7b',\n",
       " 'p_phoneme': 1.0,\n",
       " 'handle_phoneme': 'word',\n",
       " 'handle_phoneme_ambiguous': 'ignore',\n",
       " 'prepend_space_to_text': True,\n",
       " 'append_space_to_text': True,\n",
       " 'add_bos_eos_to_text': False,\n",
       " 'phonemizer_cfg': '{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttsmodel_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fcbb65e-ba65-44c8-bfb9-c87e5137c393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model2=TTSModel(**ttsmodel_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68099cea-9541-44d4-be96-5d55a2954afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_config[\"data\"][\"inference_transcript\"]=\"model_inputs/resynthesis_prompts.json\" #ToDo\n",
    "gen_config[\"data\"][\"batch_size\"]=1\n",
    "gen_config[\"data\"][\"phonemizer_cfg\"]=phonemizer_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e6465f2-868f-4da7-ab26-e9b871a9d41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemizer_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c366d31-416e-4346-8589-0db6c8cfa2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  assets/en_US_word_ipa_map.txt\n",
      "loading:  assets/es_MX_word_ipa_map.txt\n",
      "loading:  assets/de_DE_word_ipa_map.txt\n",
      "loading:  assets/en_UK_word_ipa_map.txt\n",
      "loading:  assets/es_CO_word_ipa_map.txt\n",
      "loading:  assets/es_ES_word_ipa_map.txt\n",
      "loading:  assets/fr_FR_word_ipa_map.txt\n",
      "loading:  assets/hi_HI_word_ipa_map.txt\n",
      "loading:  assets/pt_BR_word_ipa_map.txt\n",
      "loading:  assets/te_TE_word_ipa_map.txt\n",
      "Number of symbols: 439\n",
      "updating the speakers set: 7\n",
      "Initializing f0 predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing energy predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing voiced_predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing duration_predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing Speaker Regularization Component\n",
      "Initializing Speaker<>Accent Regularization Component\n",
      "HIFIGAN loading...\n",
      "RADMMMFlow(\n",
      "  (length_regulator): LengthRegulator()\n",
      "  (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
      "  (flows): ModuleList(\n",
      "    (0): FlowStep(\n",
      "      (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2-3): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4-5): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6-7): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
      ")\n",
      "/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\n",
      "Loaded pretrained decoder\n",
      "/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\n",
      "Loaded pretrained text, speaker, attention modules\n",
      "Module text_encoder not loaded from checkpoint\n",
      "Module accent_embeddings not loaded from checkpoint\n",
      "Module speaker_embeddings not loaded from checkpoint\n",
      "Module attention not loaded from checkpoint\n",
      "Module text_embeddings not loaded from checkpoint\n",
      "Module decoder not loaded from checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = TTSModel.load_from_checkpoint(checkpoint_path=attribute_model_path,\\\n",
    "                                      **ttsmodel_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f81bd7b8-01da-4c68-b214-2781a375033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  assets/en_US_word_ipa_map.txt\n",
      "loading:  assets/es_MX_word_ipa_map.txt\n",
      "loading:  assets/de_DE_word_ipa_map.txt\n",
      "loading:  assets/en_UK_word_ipa_map.txt\n",
      "loading:  assets/es_CO_word_ipa_map.txt\n",
      "loading:  assets/es_ES_word_ipa_map.txt\n",
      "loading:  assets/fr_FR_word_ipa_map.txt\n",
      "loading:  assets/hi_HI_word_ipa_map.txt\n",
      "loading:  assets/pt_BR_word_ipa_map.txt\n",
      "loading:  assets/te_TE_word_ipa_map.txt\n",
      "Number of symbols: 439\n"
     ]
    }
   ],
   "source": [
    "data_module = BaseAudioDataModule(**gen_config['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d766a7-c22c-443d-954f-4156597cccbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/LJSpeech-1.0', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'LJSpeech/ljs_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt', 'language': 'en_US', 'phonemized': True}\n",
      "processing file: datasets/opensource/LJSpeech/ljs_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/HUI-Audio-Corpus-German/Bernd_Ungerer', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'HUI-Audio-Corpus-German/Bernd_Ungerer/berndungerer_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'de_DE', 'phonemized': True}\n",
      "processing file: datasets/opensource/HUI-Audio-Corpus-German/Bernd_Ungerer/berndungerer_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/es_ES/male/tux', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/es_ES/male/tux/tux_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'es_ES', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/es_ES/male/tux/tux_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/es_MX/female/karen_savage', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/es_MX/female/karen_savage/ks_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt', 'language': 'es_MX', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/es_MX/female/karen_savage/ks_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/fr_FR/female/nadine_eckert_boulet/', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/fr_FR/female/nadine_eckert_boulet/nadineeckert_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'fr_FR', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/fr_FR/female/nadine_eckert_boulet/nadineeckert_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/indic-languages-tts-iiit-h', 'sampling_rate': '16khz_single_channel', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'indic-languages-tts-iiit-h/hi_indic_iiit_hyderbad_audiopath_text_sid_emotion_duration_train_phonemized.txt', 'language': 'hi_HI', 'phonemized': True}\n",
      "processing file: datasets/opensource/indic-languages-tts-iiit-h/hi_indic_iiit_hyderbad_audiopath_text_sid_emotion_duration_train_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/TTS-Portuguese-Corpus/', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'TTS-Portuguese-Corpus/ed_portuguese_audiopath_transcript_sid_emotion_duration_train_phonemized.txt', 'language': 'pt_BR', 'phonemized': True}\n",
      "processing file: datasets/opensource/TTS-Portuguese-Corpus/ed_portuguese_audiopath_transcript_sid_emotion_duration_train_phonemized.txt\n",
      "Number of speakers : 7\n",
      "speaker ids: {'ED-other': 0, 'hui-berndungerer-other': 1, 'indic-iiit-hyd-female-other': 2, 'ljs-other': 3, 'mailabs-karensavage-other': 4, 'mailabs-nadineeckert-other': 5, 'mailabs-tux-other': 6}\n",
      "Number of languages : 7\n",
      "language ids: {'de_DE': 0, 'en_US': 1, 'es_ES': 2, 'es_MX': 3, 'fr_FR': 4, 'hi_HI': 5, 'pt_BR': 6}\n",
      "Number of files 65197\n",
      "Include emotion ['other']: True\n",
      "Number of files after emotion filtering 65197\n",
      "Number of files after duration filtering 54096\n",
      "Dataloader initialized with no augmentations\n",
      "{'mailabs-tux-other': {'f0_median': 122.66626739501953, 'f0_mean': 124.51791381835938, 'f0_std': 24.38829803466797, 'log_f0_median': 4.809467315673828, 'log_f0_mean': 4.811684608459473, 'log_f0_std': 0.15116865932941437, 'energy_mean': 0.9740287661552429, 'energy_std': 0.06282375752925873, 'n_files': 100}, 'ed-other': {'f0_median': 150.15008544921875, 'f0_mean': 152.173828125, 'f0_std': 30.257091522216797, 'log_f0_median': 5.0116353034973145, 'log_f0_mean': 5.004810810089111, 'log_f0_std': 0.2032385766506195, 'energy_mean': 0.9661457538604736, 'energy_std': 0.03739410266280174, 'n_files': 100}, 'hui-berndungerer-other': {'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}, 'indic-iiit-hyd-female-other': {'f0_median': 265.9961242675781, 'f0_mean': 279.2513122558594, 'f0_std': 57.07512283325195, 'log_f0_median': 5.583481788635254, 'log_f0_mean': 5.612209320068359, 'log_f0_std': 0.1978427916765213, 'energy_mean': 0.9863940477371216, 'energy_std': 0.029389921575784683, 'n_files': 100}, 'ljs-other': {'f0_median': 203.92970275878906, 'f0_mean': 211.856201171875, 'f0_std': 51.960838317871094, 'log_f0_median': 5.317775249481201, 'log_f0_mean': 5.328337669372559, 'log_f0_std': 0.231437548995018, 'energy_mean': 0.9916158318519592, 'energy_std': 0.029046861454844475, 'n_files': 100}, 'mailabs-karensavage-other': {'f0_median': 207.49432373046875, 'f0_mean': 210.6017608642578, 'f0_std': 28.416366577148438, 'log_f0_median': 5.335103988647461, 'log_f0_mean': 5.341104507446289, 'log_f0_std': 0.13288341462612152, 'energy_mean': 1.0020556449890137, 'energy_std': 0.0282765943557024, 'n_files': 100}, 'mailabs-nadineeckert-other': {'f0_median': 191.37538146972656, 'f0_mean': 200.55995178222656, 'f0_std': 50.84891891479492, 'log_f0_median': 5.254236698150635, 'log_f0_mean': 5.272150993347168, 'log_f0_std': 0.23656471073627472, 'energy_mean': 0.9833611249923706, 'energy_std': 0.03346679359674454, 'n_files': 100}}\n",
      "{'mailabs-tux-other': {'f0_median': 122.66626739501953, 'f0_mean': 124.51791381835938, 'f0_std': 24.38829803466797, 'log_f0_median': 4.809467315673828, 'log_f0_mean': 4.811684608459473, 'log_f0_std': 0.15116865932941437, 'energy_mean': 0.9740287661552429, 'energy_std': 0.06282375752925873, 'n_files': 100}, 'ed-other': {'f0_median': 150.15008544921875, 'f0_mean': 152.173828125, 'f0_std': 30.257091522216797, 'log_f0_median': 5.0116353034973145, 'log_f0_mean': 5.004810810089111, 'log_f0_std': 0.2032385766506195, 'energy_mean': 0.9661457538604736, 'energy_std': 0.03739410266280174, 'n_files': 100}, 'hui-berndungerer-other': {'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}, 'indic-iiit-hyd-female-other': {'f0_median': 265.9961242675781, 'f0_mean': 279.2513122558594, 'f0_std': 57.07512283325195, 'log_f0_median': 5.583481788635254, 'log_f0_mean': 5.612209320068359, 'log_f0_std': 0.1978427916765213, 'energy_mean': 0.9863940477371216, 'energy_std': 0.029389921575784683, 'n_files': 100}, 'ljs-other': {'f0_median': 203.92970275878906, 'f0_mean': 211.856201171875, 'f0_std': 51.960838317871094, 'log_f0_median': 5.317775249481201, 'log_f0_mean': 5.328337669372559, 'log_f0_std': 0.231437548995018, 'energy_mean': 0.9916158318519592, 'energy_std': 0.029046861454844475, 'n_files': 100}, 'mailabs-karensavage-other': {'f0_median': 207.49432373046875, 'f0_mean': 210.6017608642578, 'f0_std': 28.416366577148438, 'log_f0_median': 5.335103988647461, 'log_f0_mean': 5.341104507446289, 'log_f0_std': 0.13288341462612152, 'energy_mean': 1.0020556449890137, 'energy_std': 0.0282765943557024, 'n_files': 100}, 'mailabs-nadineeckert-other': {'f0_median': 191.37538146972656, 'f0_mean': 200.55995178222656, 'f0_std': 50.84891891479492, 'log_f0_median': 5.254236698150635, 'log_f0_mean': 5.272150993347168, 'log_f0_std': 0.23656471073627472, 'energy_mean': 0.9833611249923706, 'energy_std': 0.03346679359674454, 'n_files': 100}}\n"
     ]
    }
   ],
   "source": [
    "data_module.setup(\"predict\")\n",
    "# dm.teardown(stage=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c91defc0-a41c-4412-afeb-d823b0cffa92",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextOnlyData' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextOnlyData' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "data_module.predictset.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7486ee7d-b1e5-42f6-ae10-fc83506372bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_US': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f00c8059570>,\n",
       " 'es_MX': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f022a5cd570>,\n",
       " 'de_DE': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f022a5cceb0>,\n",
       " 'en_UK': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f022a5cd030>,\n",
       " 'es_CO': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f022a5ccdc0>,\n",
       " 'es_ES': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f022a5cd240>,\n",
       " 'fr_FR': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f00c80588e0>,\n",
       " 'hi_HI': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f00c805bf40>,\n",
       " 'pt_BR': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f00c805b760>,\n",
       " 'te_TE': <tts_text_processing.grapheme_dictionary.Grapheme2PhonemeDictionary at 0x7f00c805b400>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2.tp_inference.phonemizer_backend_dict\n",
    "data_module.tp.phonemizer_backend_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd992cdd-c308-445a-9fc4-f5fe89c04498",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dl = data_module.predict_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05440126-73e6-4a2f-9f78-17845465b765",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTSModel(\n",
       "  (text_embeddings): Embedding(426, 512)\n",
       "  (text_encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): PartialConv1d(520, 520, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): InstanceNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(520, 260, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (speaker_embeddings): Embedding(21, 16)\n",
       "  (accent_embeddings): Embedding(7, 8)\n",
       "  (attention): ConvAttention(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(528, 1056, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(1056, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RADMMMFlow(\n",
       "    (length_regulator): LengthRegulator()\n",
       "    (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
       "    (flows): ModuleList(\n",
       "      (0): FlowStep(\n",
       "        (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6-7): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
       "  )\n",
       "  (decoder_criterion): RADMMMLoss(\n",
       "    (attn_loss): AttentionLoss(\n",
       "      (attn_ctc_loss): AttentionCTCLoss(\n",
       "        (log_softmax): LogSoftmax(dim=3)\n",
       "        (CTCLoss): CTCLoss()\n",
       "      )\n",
       "      (attn_bin_loss): AttentionBinarizationLoss()\n",
       "    )\n",
       "  )\n",
       "  (f0_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (f0_predictor_loss): AttributeRegressionLoss()\n",
       "  (energy_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (energy_predictor_loss): AttributeRegressionLoss()\n",
       "  (voiced_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (voiced_predictor_loss): AttributeRegressionLoss()\n",
       "  (duration_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor_loss): AttributeRegressionLoss()\n",
       "  (speaker_embed_regularization_loss): VarianceCovarianceEmbeddingRegLoss()\n",
       "  (speaker_accent_cross_regularization_loss): AttributeMinCrossCovarianceRegLoss()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c5fb8f8-fb08-4da4-9707-dc1afe82c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{d  } {v e ts i } {n  m} { a l  s,} {v a s} { i m} {d  } {k  n  } {  p,} {d ??  v a n d  t } {d i} {v y s t } {  n} {d  } {l   } {  n t} {  n} {d  } {b  a t ,} {b  s} {  } {  n} {d a s} {l a n t} {d  s} {k  n   s} {  m  } {k  m.} is NOT phonemized...\n",
      "de_DE\n",
      "de_DE|{d  } {v e ts i } {n  m}{ a l  s,} {v a s}{ i m} {d  } {k  n  } {  p,} {d ??  v a n d  t } {d i} {v y s t }{  n} {d  } {l   }{  n t}{  n} {d  } {b  a t ,} {b  s}{  }{  n} {d a s} {l a n t} {d  s} {k  n   s} {  m  } {k  m.}\n",
      "de_DE|[51, 178, 252, 0, 131, 64, 304, 114, 302, 71, 304, 252, 0, 88, 302, 164, 304, 86, 302, 40, 85, 176, 111, 25, 0, 131, 302, 40, 111, 71, 304, 86, 0, 51, 178, 252, 0, 73, 302, 150, 304, 88, 214, 146, 0, 187, 302, 164, 304, 95, 25, 0, 51, 31, 31, 146, 131, 302, 40, 88, 51, 179, 113, 176, 0, 51, 71, 304, 0, 131, 302, 135, 111, 113, 176, 214, 88, 0, 51, 178, 252, 0, 85, 302, 178, 153, 176, 266, 88, 113, 214, 88, 0, 51, 178, 252, 0, 43, 252, 302, 41, 113, 176, 25, 0, 43, 214, 111, 178, 252, 214, 88, 0, 51, 40, 111, 0, 85, 302, 40, 88, 113, 0, 51, 178, 111, 0, 73, 302, 150, 304, 88, 214, 146, 111, 0, 260, 302, 164, 86, 178, 146, 0, 73, 303, 164, 304, 86, 27]\n",
      "hui-berndungerer-other\n",
      "hui-berndungerer-other\n",
      "{'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}\n",
      "{'script': ['{d  } {v e ts i } {n  m} { a l  s,} {v a s} { i m} {d  } {k  n  } {  p,} {d ??  v a n d  t } {d i} {v y s t } {  n} {d  } {l   } {  n t} {  n} {d  } {b  a t ,} {b  s} {  } {  n} {d a s} {l a n t} {d  s} {k  n   s} {  m  } {k  m.}'], 'spk_id': tensor([1]), 'decoder_spk_id': tensor([1]), 'duration_spk_id': tensor([1]), 'f0_spk_id': tensor([1]), 'energy_spk_id': tensor([1]), 'accent_id': tensor([0]), 'text_encoded': tensor([[  0,  51, 178, 252,   0, 131,  64, 304, 114, 302,  71, 304, 252,   0,\n",
      "          88, 302, 164, 304,  86, 302,  40,  85, 176, 111,  25,   0, 131, 302,\n",
      "          40, 111,  71, 304,  86,   0,  51, 178, 252,   0,  73, 302, 150, 304,\n",
      "          88, 214, 146,   0, 187, 302, 164, 304,  95,  25,   0,  51,  31,  31,\n",
      "         146, 131, 302,  40,  88,  51, 179, 113, 176,   0,  51,  71, 304,   0,\n",
      "         131, 302, 135, 111, 113, 176, 214,  88,   0,  51, 178, 252,   0,  85,\n",
      "         302, 178, 153, 176, 266,  88, 113, 214,  88,   0,  51, 178, 252,   0,\n",
      "          43, 252, 302,  41, 113, 176,  25,   0,  43, 214, 111, 178, 252, 214,\n",
      "          88,   0,  51,  40, 111,   0,  85, 302,  40,  88, 113,   0,  51, 178,\n",
      "         111,   0,  73, 302, 150, 304,  88, 214, 146, 111,   0, 260, 302, 164,\n",
      "          86, 178, 146,   0,  73, 303, 164, 304,  86,  27,   0]]), 'idx': tensor([0]), 'speaker_f0_mean': tensor([4.9206], dtype=torch.float64), 'speaker_f0_std': tensor([0.2015], dtype=torch.float64), 'language': ['de_DE']}\n",
      "{d  } {v e ts i } {n  m} { a l  s,} {v a s} { i m} {d  } {k  n  } {  p,} {d ??  v a n d  t } {d i} {v y s t } {  n} {d  } {l   } {  n t} {  n} {d  } {b  a t ,} {b  s} {  } {  n} {d a s} {l a n t} {d  s} {k  n   s} {  m  } {k  m.} is NOT phonemized...\n",
      "de_DE\n",
      "de_DE|{d  } {v e ts i } {n  m}{ a l  s,} {v a s}{ i m} {d  } {k  n  } {  p,} {d ??  v a n d  t } {d i} {v y s t }{  n} {d  } {l   }{  n t}{  n} {d  } {b  a t ,} {b  s}{  }{  n} {d a s} {l a n t} {d  s} {k  n   s} {  m  } {k  m.}\n",
      "de_DE|[51, 178, 252, 0, 131, 64, 304, 114, 302, 71, 304, 252, 0, 88, 302, 164, 304, 86, 302, 40, 85, 176, 111, 25, 0, 131, 302, 40, 111, 71, 304, 86, 0, 51, 178, 252, 0, 73, 302, 150, 304, 88, 214, 146, 0, 187, 302, 164, 304, 95, 25, 0, 51, 31, 31, 146, 131, 302, 40, 88, 51, 179, 113, 176, 0, 51, 71, 304, 0, 131, 302, 135, 111, 113, 176, 214, 88, 0, 51, 178, 252, 0, 85, 302, 178, 153, 176, 266, 88, 113, 214, 88, 0, 51, 178, 252, 0, 43, 252, 302, 41, 113, 176, 25, 0, 43, 214, 111, 178, 252, 214, 88, 0, 51, 40, 111, 0, 85, 302, 40, 88, 113, 0, 51, 178, 111, 0, 73, 302, 150, 304, 88, 214, 146, 111, 0, 260, 302, 164, 86, 178, 146, 0, 73, 303, 164, 304, 86, 27]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/akshit/scratch/RAD-MMM/tts_lightning_modules.py:593\u001b[0m, in \u001b[0;36mTTSModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m--> 593\u001b[0m     audio_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspk_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_spk_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf0_spk_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy_spk_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduration_spk_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccent_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mf0_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeaker_f0_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mf0_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeaker_f0_std\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    602\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruct_from_batch_attributes(batch)\n",
      "File \u001b[0;32m/akshit/scratch/RAD-MMM/tts_lightning_modules.py:320\u001b[0m, in \u001b[0;36mTTSModel.sample_full\u001b[0;34m(self, raw_text, speaker_ids, decoder_speaker_ids, f0_speaker_ids, energy_speaker_ids, duration_speaker_ids, accent_ids, language, f0_mean, f0_std, shift_stats)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m energy_speaker_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     energy_speaker_ids \u001b[38;5;241m=\u001b[39m speaker_ids\n\u001b[0;32m--> 320\u001b[0m decoder_spk_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_speaker_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m f0_spk_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_speaker(f0_speaker_ids)\n\u001b[1;32m    322\u001b[0m energy_spk_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_speaker(energy_speaker_ids)\n",
      "File \u001b[0;32m/akshit/scratch/RAD-MMM/tts_lightning_modules.py:247\u001b[0m, in \u001b[0;36mTTSModel.encode_speaker\u001b[0;34m(self, spk_ids)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_speaker\u001b[39m(\u001b[38;5;28mself\u001b[39m, spk_ids):\n\u001b[0;32m--> 247\u001b[0m     spk_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspk_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spk_vecs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "model2.forward(next(iter(a_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95eeac-9593-4f69-aa37-eacd963e5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "resyn_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc627b-d8e0-4592-acc3-e3fce2e1f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some runtime error\n",
    "with open(\"/akshit/scratch/RAD-MMM/tutorials/run1/lightning_logs/version_12/hparams.yaml\", \"r\") as f:\n",
    "    hparams = yaml.safe_load(f)\n",
    "    \n",
    "# model2 = pytorch_lightning.cli.instantiate_module(TTSModel,config=hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55254e0b-8f9d-4337-9350-4e2b5df34e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model3=torch.load(attribute_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdccbda-22b0-4a16-abf9-6e79ee91e442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734de79-1886-4518-a5b6-328fc13e5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = torch.load(decoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddc29-72a4-4d9c-b594-98178e163719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3901ffc-601e-4170-b9b6-5c184c74c739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run=True, like cmd line; https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_3.html, fails with SIGSEGV\n",
    "# inp = [\"predict\", \"--config=\"+gen_config_path,\\\n",
    "#          # \"--ckpt_path=\"+attribute_model_path,\\\n",
    "#          \"--model.predict_mode=tts\", \\\n",
    "#          \"--data.inference_transcript='model_inputs/resynthesis_prompts.json'\", \\\n",
    "#          \"--model.prediction_output_dir='/akshit/scratch/RAD-MMM/tutorials/out1'\", \\\n",
    "#          \"--trainer.devices=1\", \"--data.batch_size=1\", \\\n",
    "#          \"--model.vocoder_checkpoint_path=\"+voc_model_path, \\\n",
    "#          \"--model.vocoder_config_path=\"+voc_config_path, \\\n",
    "#          \"--data.phonemizer_cfg=\"+'{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}', \\\n",
    "#          \"--model.encoders_path=\"+decoder_model_path, \\\n",
    "#          \"--model.decoder_path=\"+decoder_model_path, \\\n",
    "#          \"--model.output_directory='/akshit/scratch/RAD-MMM/tutorials/run1'\"]\n",
    "# lcli(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d335a-b5a1-49d0-b3bd-1525d0f85af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Lightning-AI/pytorch-lightning/pull/18105 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the config files\n",
    "\n",
    "# with open(gen_config_path, \"r\") as f:\n",
    "#     gen_config = yaml.safe_load(f)\n",
    "    \n",
    "#     # set config\n",
    "#     gen_config[\"ckpt_path\"] = attribute_model_path\n",
    "#     gen_config[\"model\"][\"predict_mode\"]=\"tts\"\n",
    "#     gen_config[\"model\"][\"prediction_output_dir\"]=\"/akshit/scratch/RAD-MMM/tutorials/out1\" #ToDo\n",
    "#     gen_config[\"model\"][\"vocoder_checkpoint_path\"]=voc_model_path\n",
    "#     gen_config[\"model\"][\"vocoder_config_path\"]=voc_model_path\n",
    "#     gen_config[\"model\"][\"decoder_path\"]=decoder_model_path\n",
    "#     gen_config[\"model\"][\"encoders_path\"]=decoder_model_path\n",
    "#     gen_config[\"model\"][\"output_directory\"]=\"/akshit/scratch/RAD-MMM/tutorials/run1\"\n",
    "#     gen_config[\"trainer\"][\"devices\"]=1\n",
    "    # gen_config[\"data\"][\"inference_transcript\"]=\"model_inputs/resynthesis_prompts.json\" #ToDo\n",
    "    # gen_config[\"data\"][\"batch_size\"]=1\n",
    "    # gen_config[\"data\"][\"phonemizer_cfg\"]=phonemizer_cfg\n",
    "    \n",
    "#     # set defaults\n",
    "#     gen_config[\"checkpoint_callback\"][\"filename\"]=\"latest-epoch_{epoch}-iter_{global_step:.0f}\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"monitor\"]=\"global_step\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"mode\"]= \"max\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"every_n_train_steps\"]= 3000\n",
    "#     gen_config[\"checkpoint_callback\"][\"dirpath\"]= \"/debug\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"save_top_k\"] = -1\n",
    "#     gen_config[\"checkpoint_callback\"][\"auto_insert_metric_name\"]=False\n",
    "    \n",
    "#     # linking args\n",
    "#     gen_config[\"model\"][\"phonemizer_cfg\"]=gen_config[\"data\"][\"phonemizer_cfg\"]\n",
    "#     gen_config[\"model\"][\"add_bos_eos_to_text\"]=gen_config[\"data\"][\"add_bos_eos_to_text\"]\n",
    "#     gen_config[\"model\"][\"append_space_to_text\"]=gen_config[\"data\"][\"append_space_to_text\"]\n",
    "#     gen_config[\"model\"][\"prepend_space_to_text\"]=gen_config[\"data\"][\"prepend_space_to_text\"]\n",
    "#     gen_config[\"model\"][\"handle_phoneme_ambiguous\"]=gen_config[\"data\"][\"handle_phoneme_ambiguous\"]\n",
    "#     gen_config[\"model\"][\"handle_phoneme\"]=gen_config[\"data\"][\"handle_phoneme\"]\n",
    "#     gen_config[\"model\"][\"p_phoneme\"]=gen_config[\"data\"][\"p_phoneme\"]\n",
    "#     gen_config[\"model\"][\"phoneme_dict_path\"]=gen_config[\"data\"][\"phoneme_dict_path\"]\n",
    "#     gen_config[\"model\"][\"heteronyms_path\"]=gen_config[\"data\"][\"heteronyms_path\"]\n",
    "#     gen_config[\"model\"][\"cleaner_names\"]=gen_config[\"data\"][\"cleaner_names\"]\n",
    "#     gen_config[\"model\"][\"symbol_set\"]=gen_config[\"data\"][\"symbol_set\"]\n",
    "#     gen_config[\"model\"][\"sampling_rate\"]=gen_config[\"data\"][\"sampling_rate\"]\n",
    "#     gen_config[\"checkpoint_callback\"][\"dirpath\"]=gen_config[\"model\"][\"output_directory\"]\n",
    "#     gen_config[\"trainer\"][\"default_root_dir\"]=gen_config[\"model\"][\"output_directory\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d01e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fb967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cli.model\n",
    "data_module = cli.datamodule\n",
    "trainer = cli.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_init.model\n",
    "data_module = config_init.data\n",
    "trainer = config_init.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929189ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we go through one input at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resynthesis input\n",
    "resyn_input = {\n",
    "      \"script\": \"{  h   i} {b a d} {b  n n e} {k e} {d   a n} {s  n t o} {n e} {   o d} {   u } {k  j a} {l e k  n} {p  } {t  m} {  e}\",\n",
    "      \"spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"decoder_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"duration_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"energy_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"f0_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"language\": \"hi_HI\",\n",
    "      \"emotion\": \"other\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a gradio interface to try out multiple combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a725a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
